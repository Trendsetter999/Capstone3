{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SAMSum Dialogue Summarization (MVP)\n",
        "DistilBERT encoder â†’ DistilGPT-2 decoder (EncoderDecoderModel)\n",
        "\n",
        "Note- Please install pip cell as is. May take a few min to execute. You will see red error messages after installing. Restart runtime after insalling, and proceed to next cells as normal."
      ],
      "metadata": {
        "id": "fkxXnuUS6vBW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAEZYNPP6spX"
      },
      "outputs": [],
      "source": [
        "!pip -q uninstall -y transformers tokenizers datasets huggingface_hub accelerate rouge-score >/dev/null 2>&1\n",
        "\n",
        "# 2) Install a mutually compatible bundle\n",
        "#    Note: transformers 4.45.x pairs with tokenizers 0.20.x\n",
        "!pip -q install --no-cache-dir --upgrade --upgrade-strategy eager \\\n",
        "  \"transformers==4.45.2\" \\\n",
        "  \"tokenizers==0.20.1\" \\\n",
        "  \"accelerate==0.34.2\" \\\n",
        "  \"datasets==2.20.0\" \\\n",
        "  \"huggingface_hub==0.25.2\" \\\n",
        "  \"rouge-score==0.1.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Imports, seed, device"
      ],
      "metadata": {
        "id": "ysQ4zqDb7PJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, gc, numpy as np, pandas as pd, torch\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (AutoModel, AutoConfig, GPT2LMHeadModel, EncoderDecoderModel,\n",
        "                          BertTokenizerFast, GPT2TokenizerFast,\n",
        "                          DataCollatorForSeq2Seq, Trainer, TrainingArguments)\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device, torch.cuda.get_device_name(0) if device==\"cuda\" else \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Ej_mbI7WF1",
        "outputId": "f1e212cc-6652-43cd-f165-902407b99f59"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Config knobs"
      ],
      "metadata": {
        "id": "YyPMHHcu9ev4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can bump these later if you have headroom.\n",
        "MAX_INPUT_LEN  = 512\n",
        "MAX_TARGET_LEN = 64\n",
        "\n",
        "TRAIN_SAMPLES = 1000\n",
        "VAL_SAMPLES   = 1000\n",
        "\n",
        "ENC_NAME = \"distilbert-base-uncased\"\n",
        "DEC_NAME = \"distilgpt2\"  # smaller than gpt2\n",
        "\n",
        "print(\"Config ->\", dict(MAX_INPUT_LEN=MAX_INPUT_LEN, MAX_TARGET_LEN=MAX_TARGET_LEN,\n",
        "                        TRAIN_SAMPLES=TRAIN_SAMPLES, VAL_SAMPLES=VAL_SAMPLES,\n",
        "                        ENC_NAME=ENC_NAME, DEC_NAME=DEC_NAME))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kwfNjI49zPm",
        "outputId": "de4cad4a-5c60-4339-b62d-8801d699b4b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config -> {'MAX_INPUT_LEN': 512, 'MAX_TARGET_LEN': 64, 'TRAIN_SAMPLES': 1000, 'VAL_SAMPLES': 1000, 'ENC_NAME': 'distilbert-base-uncased', 'DEC_NAME': 'distilgpt2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) Load SAMSum (with reliable fallback)"
      ],
      "metadata": {
        "id": "adpiKmZG-JhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_samsum():\n",
        "    try:\n",
        "        return load_dataset(\"knkarthick/samsum\")\n",
        "    except Exception as e:\n",
        "        print(\"Hub issue, falling back to raw JSON:\", e)\n",
        "        return load_dataset(\n",
        "            \"json\",\n",
        "            data_files={\n",
        "                \"train\":\"https://huggingface.co/datasets/samsum/resolve/main/train.json\",\n",
        "                \"validation\":\"https://huggingface.co/datasets/samsum/resolve/main/validation.json\",\n",
        "                \"test\":\"https://huggingface.co/datasets/samsum/resolve/main/test.json\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "ds = load_samsum()\n",
        "\n",
        "# Trim for MVP speed\n",
        "train_small = ds[\"train\"].select(range(min(TRAIN_SAMPLES, len(ds[\"train\"]))))\n",
        "val_small   = ds[\"validation\"].select(range(min(VAL_SAMPLES, len(ds[\"validation\"]))))\n",
        "print(ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WRqzdNgEfvB",
        "outputId": "b84488f7-9808-4442-9b3b-4eb6df41cd43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_token.py:90: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 14731\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 818\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 819\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) Tokenizers & preprocessing"
      ],
      "metadata": {
        "id": "NmzjJqKEEk3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tok = BertTokenizerFast.from_pretrained(ENC_NAME)\n",
        "gpt2_tok  = GPT2TokenizerFast.from_pretrained(DEC_NAME)\n",
        "\n",
        "# Ensure PAD token for GPT-2 family\n",
        "if gpt2_tok.pad_token is None:\n",
        "    gpt2_tok.pad_token = gpt2_tok.eos_token   # simple & safe\n",
        "\n",
        "def preprocess(batch):\n",
        "    # Encoder inputs (DistilBERT)\n",
        "    model_inputs = bert_tok(\n",
        "        batch[\"dialogue\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_INPUT_LEN\n",
        "    )\n",
        "    # Decoder targets (DistilGPT-2)\n",
        "    with gpt2_tok.as_target_tokenizer():\n",
        "        labels = gpt2_tok(\n",
        "            batch[\"summary\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_TARGET_LEN\n",
        "        )[\"input_ids\"]\n",
        "\n",
        "    # Mask PAD with -100 so it doesn't contribute to loss\n",
        "    labels = [\n",
        "        [(tid if tid != gpt2_tok.pad_token_id else -100) for tid in seq]\n",
        "        for seq in labels\n",
        "    ]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "proc_train = train_small.map(preprocess, batched=True, remove_columns=train_small.column_names)\n",
        "proc_val   = val_small.map(preprocess,   batched=True, remove_columns=val_small.column_names)\n",
        "proc = DatasetDict({\"train\": proc_train, \"validation\": proc_val})\n",
        "proc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "85e1c6bbbc1c4a93a7a3690feedb767a",
            "dd81ee8d4f77422488223810216aeefa",
            "afeb11a6d3174b9291e23ac74dc4f939",
            "0af30f42a5be4010816585dea3fc4b53",
            "6dcb1ce5d1064108809d0e7e5b4cf6a5",
            "65475bccbd6b48bf8b927431eae8a723",
            "33f45bea658846ef81d4f9df5eb8590c",
            "f9158ccf6e2548aaa38ba97308be2c31",
            "e5cd0ae6f43544e09d2d6faed5848d1b",
            "58fb870edb5446fe8d11a09255939d44",
            "4ea1be4a046f49f5bd8cc6546c643f03"
          ]
        },
        "id": "7C9hpOdrEoCb",
        "outputId": "d1b9a30a-07af-4a79-8daf-d04283087c76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85e1c6bbbc1c4a93a7a3690feedb767a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F) Build model (DistilBERT â†’ DistilGPT-2) + memory savers"
      ],
      "metadata": {
        "id": "2BLs4M2lEwOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder = AutoModel.from_pretrained(ENC_NAME)\n",
        "\n",
        "# Decoder config with cross-attention enabled\n",
        "dec_config = AutoConfig.from_pretrained(DEC_NAME)\n",
        "dec_config.is_decoder = True\n",
        "dec_config.add_cross_attention = True\n",
        "\n",
        "# Decoder\n",
        "decoder = GPT2LMHeadModel.from_pretrained(DEC_NAME, config=dec_config)\n",
        "decoder.resize_token_embeddings(len(gpt2_tok))\n",
        "\n",
        "# Compose encoder-decoder\n",
        "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
        "\n",
        "# Special tokens + generation defaults\n",
        "model.config.decoder_start_token_id = gpt2_tok.eos_token_id  # GPT-2 has no BOS; EOS works fine\n",
        "model.config.eos_token_id = gpt2_tok.eos_token_id\n",
        "model.config.pad_token_id = gpt2_tok.pad_token_id\n",
        "model.config.max_length = MAX_TARGET_LEN\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.num_beams = 2\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Memory savers\n",
        "model.gradient_checkpointing_enable()  # big VRAM win\n",
        "model.config.use_cache = False         # avoid cache during training\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuhLyraEEyfi",
        "outputId": "283c2f1f-6b8e-4146-bc3b-8355aa2e1792"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "EncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoderModel(\n",
              "  (encoder): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-5): 6 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2SdpaAttention(\n",
              "            (c_attn): Conv1D(nf=2304, nx=768)\n",
              "            (c_proj): Conv1D(nf=768, nx=768)\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (crossattention): GPT2SdpaAttention(\n",
              "            (c_attn): Conv1D(nf=1536, nx=768)\n",
              "            (q_attn): Conv1D(nf=768, nx=768)\n",
              "            (c_proj): Conv1D(nf=768, nx=768)\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D(nf=3072, nx=768)\n",
              "            (c_proj): Conv1D(nf=768, nx=3072)\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "G) Data collator (fp16-friendly)"
      ],
      "metadata": {
        "id": "77jPtd-XE5R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=bert_tok,\n",
        "    model=model,\n",
        "    padding=\"longest\",\n",
        "    pad_to_multiple_of=8,   # helps fp16\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "data_collator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1OrGRhkE7ov",
        "outputId": "6129728f-c6d9-4d12-dc58-5ea08b2f520e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataCollatorForSeq2Seq(tokenizer=BertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}, model=EncoderDecoderModel(\n",
              "  (encoder): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-5): 6 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2SdpaAttention(\n",
              "            (c_attn): Conv1D(nf=2304, nx=768)\n",
              "            (c_proj): Conv1D(nf=768, nx=768)\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (crossattention): GPT2SdpaAttention(\n",
              "            (c_attn): Conv1D(nf=1536, nx=768)\n",
              "            (q_attn): Conv1D(nf=768, nx=768)\n",
              "            (c_proj): Conv1D(nf=768, nx=768)\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D(nf=3072, nx=768)\n",
              "            (c_proj): Conv1D(nf=768, nx=3072)\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              "), padding='longest', max_length=None, pad_to_multiple_of=8, label_pad_token_id=-100, return_tensors='pt')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "H) ROUGE metric (robust to HF outputs)"
      ],
      "metadata": {
        "id": "OicO4SkOFAWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_scorer_fn = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    import numpy as np\n",
        "    preds, labels = eval_pred\n",
        "    # Some HF versions return a tuple (preds, other)\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    preds  = np.asarray(preds)\n",
        "    labels = np.asarray(labels)\n",
        "    # Replace masked positions so we can decode\n",
        "    labels = np.where(labels != -100, labels, gpt2_tok.pad_token_id)\n",
        "\n",
        "    decoded_preds  = gpt2_tok.batch_decode(preds,  skip_special_tokens=True)\n",
        "    decoded_labels = gpt2_tok.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    agg = scoring.BootstrapAggregator()\n",
        "    for ref, hyp in zip(decoded_labels, decoded_preds):\n",
        "        agg.add_scores(rouge_scorer_fn.score(ref, hyp))\n",
        "\n",
        "    res = agg.aggregate()\n",
        "    return {k: round(v.mid.fmeasure*100, 2) for k,v in res.items()}"
      ],
      "metadata": {
        "id": "w836bEvPFDwq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I) Trainer setup (epoch style, small VRAM)"
      ],
      "metadata": {
        "id": "HcjaLqFvFIM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "bs = 2\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./distilbert_distilgpt2_samsum_mvp\",\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=50,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    save_total_limit=1,\n",
        "\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=MAX_TARGET_LEN,\n",
        "    generation_num_beams=2,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=proc[\"train\"],\n",
        "    eval_dataset=proc[\"validation\"],\n",
        "    tokenizer=bert_tok,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "start_id = gpt2_tok.bos_token_id if gpt2_tok.bos_token_id is not None else gpt2_tok.eos_token_id\n",
        "\n",
        "# keep model.config populated\n",
        "model.config.decoder_start_token_id = start_id\n",
        "model.config.bos_token_id          = start_id\n",
        "model.config.eos_token_id          = gpt2_tok.eos_token_id\n",
        "model.config.pad_token_id          = gpt2_tok.pad_token_id\n",
        "\n",
        "# ðŸ”‘ keep generation_config in sync (Transformers >= 4.27)\n",
        "gen = model.generation_config\n",
        "gen.decoder_start_token_id = model.config.decoder_start_token_id\n",
        "gen.bos_token_id           = model.config.bos_token_id\n",
        "gen.eos_token_id           = model.config.eos_token_id\n",
        "gen.pad_token_id           = model.config.pad_token_id\n",
        "gen.max_length             = MAX_TARGET_LEN\n",
        "gen.num_beams              = 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fX_LfVzFLuE",
        "outputId": "75fceb77-0b35-4db2-fa8b-2aab8bbf164a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J) Train and Evaluate"
      ],
      "metadata": {
        "id": "q9eCsH6xGUgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()\n",
        "trainer.save_model()\n",
        "metrics = trainer.evaluate()\n",
        "print(\"ROUGE on validation:\", {k: round(v, 2) if isinstance(v, float) else v for k, v in metrics.items()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "zPmDGw2AGZIh",
        "outputId": "744d850f-efb5-443b-da74-fa41d6bd8a7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [93/93 01:59, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.144200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 2, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 05:25]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE on validation: {'eval_loss': 3.86, 'eval_rouge1': 14.96, 'eval_rouge2': 2.06, 'eval_rougeL': 12.26, 'eval_runtime': 326.77, 'eval_samples_per_second': 2.5, 'eval_steps_per_second': 1.25, 'epoch': 2.98}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K) Sample generations + lightweight eval (fallback)"
      ],
      "metadata": {
        "id": "e1jET4J-IWnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K) Sample generations + lightweight eval (fixed version)\n",
        "model.eval()\n",
        "model.config.use_cache = True  # faster inference\n",
        "\n",
        "def generate_summary(dialogue):\n",
        "    # Encode input text\n",
        "    enc = bert_tok(\n",
        "        dialogue,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_INPUT_LEN\n",
        "    )\n",
        "    # Keep only tensors the model actually uses (avoid token_type_ids error)\n",
        "    enc = {k: v.to(device) for k, v in enc.items() if k in (\"input_ids\", \"attention_mask\")}\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        gen = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=MAX_TARGET_LEN,\n",
        "            num_beams=2,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    return gpt2_tok.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "# --- Show a few validation examples ---\n",
        "for i in range(3):\n",
        "    d = ds[\"validation\"][i][\"dialogue\"]\n",
        "    ref = ds[\"validation\"][i][\"summary\"]\n",
        "    pred = generate_summary(d)\n",
        "    print(f\"\\n=== Example {i+1} ===\")\n",
        "    print(\"REF :\", ref)\n",
        "    print(\"PRED:\", pred)\n",
        "\n",
        "# --- Lightweight manual ROUGE on small slice (no dependence on Trainer.evaluate) ---\n",
        "small = ds[\"validation\"].select(range(50))\n",
        "agg = scoring.BootstrapAggregator()\n",
        "\n",
        "for ex in small:\n",
        "    pred = generate_summary(ex[\"dialogue\"])\n",
        "    agg.add_scores(rouge_scorer_fn.score(ex[\"summary\"], pred))\n",
        "\n",
        "lite_res = agg.aggregate()\n",
        "print(\"\\nLite ROUGE on 50 examples:\", {k: round(v.mid.fmeasure * 100, 2) for k, v in lite_res.items()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUh48DzYId4a",
        "outputId": "f8658663-80bc-453b-ddd5-519deffb5777"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Example 1 ===\n",
            "REF : A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
            "PRED: Sophie is going to a party at the end of the week. She's going to the party at 8 pm. Â  Â Â   Â  Â  Â  Â  Â  Â  Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n",
            "Â  Â Â Â  Â Â Â Â  Â \n",
            "\n",
            "=== Example 2 ===\n",
            "REF : Emma and Rob love the advent calendar. Lauren fits inside calendar various items, for instance, small toys and Christmas decorations. Her children are excited whenever they get the calendar.\n",
            "PRED: Karen is going to a party at the end of the week. Â   Â  She will be there for a few hours.  Â Â     Â   Â Â Â   Ã‚   Â Â    Â  Â    Â  Â  Â  Â    Â  Â  Â  Â  Â  Â  Â  Â   \n",
            "Â  Â  Â  Â Â  Â \n",
            "\n",
            "=== Example 3 ===\n",
            "REF : Madison is pregnant but she doesn't want to talk about it. Patricia Stevens got married and she thought she was pregnant. \n",
            "PRED: Sophie is going to a party at the end of the week. She's going to have a drink. Â    Â Â  Â  Â  Â \n",
            "\n",
            "Lite ROUGE on 50 examples: {'rouge1': np.float64(14.05), 'rouge2': np.float64(1.44), 'rougeL': np.float64(11.8)}\n"
          ]
        }
      ]
    }
  ]
}